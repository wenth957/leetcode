{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 5.1 简单层的实现\r\n",
    "- 反向传播利用计算图理解\r\n",
    "- 基本原理是链式法则"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 乘法层\r\n",
    "class MulLayer:\r\n",
    "    def __init__(self):\r\n",
    "        self.x = None\r\n",
    "        self.y = None\r\n",
    "    \r\n",
    "    def forward(self,x,y):\r\n",
    "        self.x = x\r\n",
    "        self.y = y\r\n",
    "        out = x*y\r\n",
    "        return  out\r\n",
    "    \r\n",
    "    def  backward(self,dout):\r\n",
    "        dx = dout*self.y\r\n",
    "        dy = dout*self.x\r\n",
    "\r\n",
    "        return dx,dy\r\n",
    "\r\n",
    "# 测试\r\n",
    "apple_price = 100\r\n",
    "apple_num = 2\r\n",
    "tax_consumer = 1.1\r\n",
    "apple = MulLayer() #多层创建多个实例\r\n",
    "tax = MulLayer()\r\n",
    "# 正向传播\r\n",
    "price = apple.forward(apple_price,apple_num)\r\n",
    "price_tax = tax.forward(price,tax_consumer)\r\n",
    "# 反向传播\r\n",
    "dprice_tax = 1\r\n",
    "dprice,dtax = tax.backward(dprice_tax)\r\n",
    "dapple_price,dapple_num = apple.backward(dprice)\r\n",
    "print(dapple_price,dapple_num,dtax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 加法层的实现\r\n",
    "class AddLayer:\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def forward(self,x,y):\r\n",
    "        out = x+y\r\n",
    "        return out\r\n",
    "    \r\n",
    "    def backward(self,dout):\r\n",
    "        dx  = dout*1\r\n",
    "        dy = dout*1\r\n",
    "        return dx, dy\r\n",
    "\r\n",
    "# 测试\r\n",
    "apple_price = 100\r\n",
    "apple_num = 2\r\n",
    "tax_con = 1.1\r\n",
    "orange_price = 150\r\n",
    "orange_num  =3\r\n",
    "# forward\r\n",
    "apple = MulLayer()\r\n",
    "orange = MulLayer()\r\n",
    "sum_fruit = AddLayer()\r\n",
    "tax = MulLayer()\r\n",
    "price_app = apple.forward(apple_price,apple_num)\r\n",
    "price_ora = orange.forward(orange_price,orange_num)\r\n",
    "price_fruit = sum_fruit.forward(price_app,price_ora)\r\n",
    "price_tax = tax.forward(price_fruit,tax_con)\r\n",
    "print(price_tax)\r\n",
    "# backward\r\n",
    "dprice_tax =1\r\n",
    "dprice_fruit,dtax_con = tax.backward(dprice_tax)\r\n",
    "print(dprice_fruit)\r\n",
    "dprice_app,dprice_org = sum_fruit.backward(dprice_fruit)\r\n",
    "dapple_price,dapple_num = apple.backward(dprice_app)\r\n",
    "dorange_price,dorange_num = orange.backward(dprice_org)\r\n",
    "print(dapple_price,dapple_num,dorange_price,dorange_num,dprice_fruit,dtax_con)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.2 神经网络激活层的实现\r\n",
    "- ReLU激活函数\r\n",
    "- Sigmoid激活函数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ReLu激活函数\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "class ReLu:\r\n",
    "    '''激活函数的反向传播'''\r\n",
    "    def __init__(self):\r\n",
    "        self.mask = None\r\n",
    "    \r\n",
    "    def forward(self,x):\r\n",
    "        self.mask = (x<=0)\r\n",
    "        out = x.copy()\r\n",
    "        out[self.mask] = 0\r\n",
    "        return out\r\n",
    "    \r\n",
    "    def backward(self,dout):\r\n",
    "        dout[self.mask] = 0\r\n",
    "        dx = dout\r\n",
    "        return dx\r\n",
    "\r\n",
    "x = np.array([[1.0,-0.5],[-2.0,3.0]])\r\n",
    "print(x)\r\n",
    "f = ReLu()\r\n",
    "out = f.forward(x)\r\n",
    "dout  = np.ones_like(x)\r\n",
    "dx = f.backward(dout)\r\n",
    "print(out)\r\n",
    "print(dx)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sigmoid激活函数\r\n",
    "\r\n",
    "class Sigmoid:\r\n",
    "\r\n",
    "    def __init__(self):\r\n",
    "        self.out = None\r\n",
    "\r\n",
    "    def forward(self,x):\r\n",
    "        out = 1/(1+np.exp(-x))\r\n",
    "        self.out = out\r\n",
    "\r\n",
    "        return out\r\n",
    "    \r\n",
    "    def backward(self,dout):\r\n",
    "        dx = dout*self.out*(1.0-self.out)\r\n",
    "\r\n",
    "        return dx  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.3 输入输出层的实现\r\n",
    "- Affine层：线性变换\r\n",
    "- Softmax层：多分类"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Y=XW+B\r\n",
    "# 对于XW的梯度，最后与X,W形状相同\r\n",
    "# 对于B的梯度，按axis=0加总，因为B加到Y每一行上面，所以B对结果L的影响应该是Y每一行的影响之和\r\n",
    "\r\n",
    "class Affine:\r\n",
    "    def __init__(self,W,b):\r\n",
    "        self.W=W\r\n",
    "        self.b = b\r\n",
    "        self.x =None\r\n",
    "        self.dW = None\r\n",
    "        self.db = None\r\n",
    "    \r\n",
    "    def forward(self,x):\r\n",
    "        self.x = x\r\n",
    "        out = np.dot(x,self.W) +self.b\r\n",
    "\r\n",
    "        return out\r\n",
    "    \r\n",
    "    def backward(self,dout):\r\n",
    "        dx = np.dot(dout,self.W.T)\r\n",
    "        self.dW = np.dot(self.x.T,dout)\r\n",
    "        self.db  = np.sum(dout,axis=0)\r\n",
    "\r\n",
    "        return dx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Softmax层\r\n",
    "def cross_entropy_error(y,t):\r\n",
    "    if y.ndim == 1:\r\n",
    "        t = t.reshape(1,t.size)\r\n",
    "        y = y.reshape(1,y.size)\r\n",
    "    \r\n",
    "    batch_size = y.shape[0]\r\n",
    "    return  -np.sum(t*np.log(y+1e-7))/batch_size\r\n",
    "\r\n",
    "\r\n",
    "class SoftmaxWithLoss:\r\n",
    "\r\n",
    "    def __init__(self):\r\n",
    "        self.loss =None\r\n",
    "        self.y = None\r\n",
    "        self.t = None\r\n",
    "\r\n",
    "    def softmax(x):\r\n",
    "        a = np.max(x)\r\n",
    "        exp_x = np.exp(x-a)\r\n",
    "        exp_sum = np.sum(exp_x)\r\n",
    "        y = exp_x/exp_sum\r\n",
    "        return y\r\n",
    "\r\n",
    "    def cross_entropy_error(y,t):\r\n",
    "\r\n",
    "        if y.ndim == 1:\r\n",
    "            t = t.reshape(1,t.size)\r\n",
    "            y = y.reshape(1,y.size)\r\n",
    "    \r\n",
    "        batch_size = y.shape[0]\r\n",
    "        return  -np.sum(t*np.log(y+1e-7))/batch_size\r\n",
    "    \r\n",
    "    def forward(self,x,t):\r\n",
    "        self.t = t \r\n",
    "        self.y = softmax(x)\r\n",
    "        self.loss = cross_entropy_error(self.y,self.t)\r\n",
    "        return self.loss\r\n",
    "    \r\n",
    "    def backward(self,dout=1):\r\n",
    "        batch_size = self.t.shape[0] #批的大小\r\n",
    "        if  self.t.size == self.y.size:\r\n",
    "            dx = (self.y - self.t)/batch_size \r\n",
    "            #最后推导出来的公式/数据个数 将单个数据的误差传递给前面层\r\n",
    "            # 如果有N份数据，那么计算的交叉熵对于x的导数，应该在一份数据的基础上1/N\r\n",
    "        else:\r\n",
    "            # 不是独热编码\r\n",
    "            dx = self.y.copy()\r\n",
    "            dx[np.arange(batch_size),self.t] -= 1\r\n",
    "            # 花式索引，然后计算误差：概率-1\r\n",
    "            dx = dx/batch_size\r\n",
    "        return dx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.4 误差反向传播法的实现\r\n",
    "- 两层网络的实现"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import OrderedDict\r\n",
    "\r\n",
    "\r\n",
    "def numerical_gradient(f,x):\r\n",
    "    '''梯度'''\r\n",
    "    h =1e-4\r\n",
    "    grad = np.zeros_like(x)\r\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])   #多维迭代器\r\n",
    "    # 不断按行迭代，计算每一行的梯度，直到完成\r\n",
    "    while not it.finished:\r\n",
    "        idx = it.multi_index\r\n",
    "        tmp_val = x[idx]\r\n",
    "        x[idx] = tmp_val +h\r\n",
    "        fxh1 = f(x)\r\n",
    "        x[idx] = tmp_val -h\r\n",
    "        fxh2 = f(x)\r\n",
    "        grad[idx] = (fxh1-fxh2)/(2*h)\r\n",
    "        x[idx] = tmp_val\r\n",
    "        it.iternext() \r\n",
    "    \r\n",
    "    return grad\r\n",
    "\r\n",
    "\r\n",
    "class TwoLayerNet:\r\n",
    "\r\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_std = 0.01):\r\n",
    "        # 初始化参数\r\n",
    "        self.params = {}\r\n",
    "        self.params['W1'] = weight_std*np.random.randn(input_size,hidden_size)\r\n",
    "        self.params['b1'] =weight_std*np.random.randn(hidden_size)\r\n",
    "        self.params['W2'] = weight_std*np.random.randn(hidden_size,output_size)\r\n",
    "        self.params['b2'] = weight_std*np.random.randn(output_size)\r\n",
    "\r\n",
    "        # 生成层\r\n",
    "        self.layers = OrderedDict()\r\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'],self.params['b1'])\r\n",
    "        self.layers['ReLu1'] = ReLu()\r\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'],self.params['b2'])\r\n",
    "        # 输出层\r\n",
    "        self.lastLayer = SoftmaxWithLoss()\r\n",
    "    \r\n",
    "    def predict(self,x):\r\n",
    "        for layer in self.layers.values():  # 按有序字典循环层\r\n",
    "            x = layer.forward(x)              # 每层都正向传播，得到得分\r\n",
    "        \r\n",
    "        return x\r\n",
    "    \r\n",
    "    def loss(self,x,t):\r\n",
    "        y = self.predict(x)\r\n",
    "        return self.lastLayer.forward(y,t)\r\n",
    "    \r\n",
    "    def  accuracy(self,x,t):\r\n",
    "        y= self.predict(x)\r\n",
    "        y = np.argmax(y,axis=1)\r\n",
    "        if  t.ndim != 1:     # 独热编码\r\n",
    "            t = np.argmax(t,axis=1)\r\n",
    "        \r\n",
    "        accuracy = np.sum(y==t)/float(x.shape[0])\r\n",
    "        # 如果是标签，那么最大的参数索引就是应该和t相等\r\n",
    "        # 如果是独热编码，应该返回等于1的索引，与预测结果比较\r\n",
    "        return accuracy\r\n",
    "    \r\n",
    "    def numerical_gradient(self,x,t):\r\n",
    "        loss_W = lambda W: self.loss(x,t)\r\n",
    "        \r\n",
    "        grads = {}\r\n",
    "        grads['W1'] = numerical_gradient(loss_W,self.params['W1'])\r\n",
    "        grads['b1']  = numerical_gradient(loss_W,self.params['b1'])\r\n",
    "        grads['W2']  = numerical_gradient(loss_W,self.params['W2'])\r\n",
    "        grads['b2']  = numerical_gradient(loss_W,self.params['b2'])\r\n",
    "        \r\n",
    "        return  grads\r\n",
    "        \r\n",
    "    def gradient(self,x,t):\r\n",
    "        # forward\r\n",
    "        self.loss(x,t)\r\n",
    "        # backward\r\n",
    "        dout =1\r\n",
    "        dout = self.lastLayer.backward(dout)\r\n",
    "\r\n",
    "        layers = list(self.layers.values())\r\n",
    "        layers.reverse()  # 列表反向\r\n",
    "        for layer in layers:\r\n",
    "            dout = layer.backward(dout)\r\n",
    "    \r\n",
    "        grads={ }\r\n",
    "        grads['W1'] = self.layers['Affine1'].dW\r\n",
    "        grads['b1'] = self.layers['Affine1'].db\r\n",
    "        grads['W2'] = self.layers['Affine2'].dW\r\n",
    "        grads['b2'] = self.layers['Affine2'].db\r\n",
    "\r\n",
    "        return grads\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 梯度确认\r\n",
    "import sys,os\r\n",
    "sys.path.append(os.pardir)\r\n",
    "import numpy as np\r\n",
    "from analysis.mnist import load_mnist\r\n",
    "\r\n",
    "(x_train,t_train),(x_test,t_test) = load_mnist(normalize=True,one_hot_label=True)\r\n",
    "network = TwoLayerNet(784,50,10)\r\n",
    "\r\n",
    "x_batch = x_train[:3]\r\n",
    "t_batch = t_train[:3]\r\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\r\n",
    "grad_back = network.gradient(x_batch,t_batch)\r\n",
    "# 求绝对误差\r\n",
    "for key in grad_numerical.keys():\r\n",
    "    print(grad_numerical[key].shape)\r\n",
    "    print(grad_back[key].shape)\r\n",
    "    diff = np.average(np.abs(grad_numerical[key]-grad_back[key]))\r\n",
    "    print(key+ \" : \" + str(diff))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 误差反向传播的学习\r\n",
    "iters_num = 10000\r\n",
    "train_size = x_train.shape[0]\r\n",
    "batch_size = 100\r\n",
    "learning_rate  = 0.1\r\n",
    "train_loss_list = []\r\n",
    "train_acc_list = []\r\n",
    "test_acc_list = []\r\n",
    "iter_per_epoch = max(train_size/batch_size,1)\r\n",
    "(x_train,t_train),(x_test,t_test) = load_mnist(normalize=True,one_hot_label=True)\r\n",
    "network = TwoLayerNet(784,50,10)\r\n",
    "\r\n",
    "for i in range(iters_num):\r\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\r\n",
    "    x_batch = x_train[batch_mask]\r\n",
    "    t_batch =  t_train[batch_mask]\r\n",
    "\r\n",
    "    grad = network.gradient(x_batch,t_batch)\r\n",
    "    # 更新\r\n",
    "    for key in (\"W1\",'b1','W2','b2'):\r\n",
    "        network.params[key] -= learning_rate*grad[key]\r\n",
    "    \r\n",
    "    loss = network.loss(x_batch,t_batch)\r\n",
    "    train_loss_list.append(loss)\r\n",
    "    if i%iter_per_epoch ==0:\r\n",
    "        train_acc = network.accuracy(x_train,t_train)\r\n",
    "        test_acc = network.accuracy(x_test,t_test)\r\n",
    "        train_acc_list.append(train_acc)\r\n",
    "        test_acc_list.append(test_acc)\r\n",
    "        print(train_acc,test_acc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 可视化\r\n",
    "import matplotlib.pylab as plt\r\n",
    "x  = np.arange(len(train_acc_list))\r\n",
    "y1  = np.array(train_acc_list)\r\n",
    "y2 = np.array(test_acc_list)\r\n",
    "plt.plot(x,y1,label='train_acc')\r\n",
    "plt.plot(x,y2,label='test_acc',linestyle='--')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}