{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 3.1 非线性函数的构造\r\n",
    "- 任意曲线都可以用激活函数来构造\r\n",
    "- 任意曲线都是一些激活函数的和 \r\n",
    "- 这些激活函数 偏置和权重不同\r\n",
    "-  两个ReLU函数才能构造一个阶跃函数或者sigmoid函数\r\n",
    "-  所以同样情况下ReLU的激活函数(神经元)需要增加一倍"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 激活函数\r\n",
    "# 阶跃函数的实现\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "def  step_func(x):\r\n",
    "    if  x >0:\r\n",
    "        return 1\r\n",
    "    elif x<=0:\r\n",
    "        return 0\r\n",
    "# 测试\r\n",
    "step_func(11)\r\n",
    "x = np.array([11,11])\r\n",
    "# x >0\r\n",
    "# step_func([11,11])\r\n",
    "# TypeError: '>' not supported between instances of 'list' and 'int'\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 利用numpy数组改进\r\n",
    "import numpy as np  \r\n",
    "import matplotlib.pylab as plt\r\n",
    "\r\n",
    "def step_function(x):\r\n",
    "    '''阶跃函数的实现'''\r\n",
    "    y = x>0  # 得到的布尔数组\r\n",
    "    return y.astype(np.int)   #转换成0，1\r\n",
    "\r\n",
    "x = np.arange(-5.0,5.0,0.1)\r\n",
    "y = step_function(x)\r\n",
    "# 可视化\r\n",
    "plt.plot(x,y)\r\n",
    "plt.ylim(-0.1,1.1)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sigmoid(x):\r\n",
    "    return 1/(1+np.exp(-x))\r\n",
    "\r\n",
    "x = np.arange(-5,5,0.1)\r\n",
    "y = sigmoid(x)\r\n",
    "plt.plot(x,y)\r\n",
    "plt.ylim(-0.1,1.1)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ReLU 线性整流函数\r\n",
    "def ReLU(x):\r\n",
    "    return np.maximum(0,x)\r\n",
    "\r\n",
    "# 测试\r\n",
    "ReLU(2)\r\n",
    "x = np.arange(-5,5,0.1)\r\n",
    "y = ReLU(x)\r\n",
    "plt.plot(x,y)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.2 多维数组"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "A= np.array([[[1,2],[3,4],[5,6]],[[2,2],[3,3],[4,4]]])\r\n",
    "A.shape  # 维度从外到内读"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.3 神经网络的实现"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 矩阵乘法\r\n",
    "X = np.array([1,2])\r\n",
    "W = np.array([[1,3,5],[2,4,6]])  # 第一个是x1的权重，第二个是x2的权重\r\n",
    "Y = np.dot(X,W)\r\n",
    "print(Y)\r\n",
    "# 隐藏层1的实现\r\n",
    "X = np.array([1.0,0.5])\r\n",
    "W1 = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])\r\n",
    "B1 = np.array([0.1,0.2,0.3])\r\n",
    "print(X.shape)\r\n",
    "print(W1.shape)\r\n",
    "print(B1.shape)\r\n",
    "A1 = np.dot(X,W1)+B1\r\n",
    "Z1 = sigmoid(A1)\r\n",
    "print(A1)\r\n",
    "print(Z1)\r\n",
    "# 隐藏层2的实现\r\n",
    "W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])\r\n",
    "B2 =  np.array([0.1,0.2])\r\n",
    "print(Z1.shape)\r\n",
    "print(W2.shape)\r\n",
    "print(B2.shape)\r\n",
    "A2 = np.dot(Z1,W2) + B2\r\n",
    "Z2 = sigmoid(A2)\r\n",
    "# 输出层的实现\r\n",
    "W3 = np.array([[0.1,0.3],[0.2,0.4]])\r\n",
    "B3 = np.array([0.1,0.2])\r\n",
    "A3 = np.dot(Z2,W3)+B3\r\n",
    "# 输出层激活函数\r\n",
    "# 回归问题使用恒等函数，分类问题使用sigmoid函数、多分类用softmax函数\r\n",
    "def identity_func(x):\r\n",
    "    '''输出层激活函数'''\r\n",
    "    return x\r\n",
    "\r\n",
    "def _softmax(x):\r\n",
    "    exp_x = np.exp(x)\r\n",
    "    sum_exp = np.sum(exp_x)\r\n",
    "    y = exp_x /sum_exp\r\n",
    "    return y\r\n",
    "\r\n",
    "# softmax需要解决  某个指数，数据过大问题\r\n",
    "\r\n",
    "def softmax(x):\r\n",
    "    a = np.max(x)\r\n",
    "    exp_x = np.exp(x-a)\r\n",
    "    exp_sum = np.sum(exp_x)\r\n",
    "    y = exp_x/exp_sum\r\n",
    "    return y\r\n",
    "\r\n",
    "Y = identity_func(A3) \r\n",
    "print(Y)\r\n",
    "\r\n",
    "# 测试\r\n",
    "x = np.array([1010,1000,990])\r\n",
    "y1 = _softmax(x)\r\n",
    "y2 = softmax(x)\r\n",
    "print(y1)\r\n",
    "print(y2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "面向对象形式\r\n",
    "\r\n",
    "class network:\r\n",
    "\r\n",
    "    def __init__(self,W1,b1,W2,b2,W3,b3):\r\n",
    "        network = {}\r\n",
    "        self.W1 = W1\r\n",
    "        self.W2 = W2\r\n",
    "        self.W3 = W3\r\n",
    "        self.b1 = b1\r\n",
    "        self.b2 = b2\r\n",
    "        self.b3 = b3\r\n",
    "\r\n",
    "    def forward(self,x):\r\n",
    "        a1 = np.dot(x,self.W1)+self.b1\r\n",
    "        z1 = sigmoid(a1)\r\n",
    "        a2 = np.dot(z1,self.W2) + self.b2\r\n",
    "        z2 = sigmoid(a2)\r\n",
    "        a3 = np.dot(z2,self.W3) +self.b3\r\n",
    "        y = identity_func(a3)\r\n",
    "        return y\r\n",
    "\r\n",
    "x = np.array([1.0,0.5])\r\n",
    "W1 = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])\r\n",
    "b1 = np.array([0.1,0.2,0.3])\r\n",
    "W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])\r\n",
    "b2 = np.array([0.1,0.2])\r\n",
    "W3 = np.array([[0.1,0.3],[0.2,0.4]])\r\n",
    "b3 = np.array([0.1,0.2])\r\n",
    "network = network(W1,b1,W2,b2,W3,b3)\r\n",
    "output = network.forward(x)\r\n",
    "print(output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.4 手写数字识别"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 下载mnist数据集\r\n",
    "import sys,os\r\n",
    "sys.path.append(os.pardir)\r\n",
    "from res.mnist import load_mnist\r\n",
    "from PIL import Image\r\n",
    "\r\n",
    "# (x_train, t_train),(x_test,t_test) = load_mnist(flatten=True,normalize=False)\r\n",
    "# print(x_train.shape)\r\n",
    "# print(t_train.shape)\r\n",
    "# print(x_test.shape)\r\n",
    "# print(t_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 展示第一张照片\r\n",
    "def img_show(img):\r\n",
    "    pil_img = Image.fromarray(np.uint8(img))\r\n",
    "    pil_img.show()\r\n",
    "\r\n",
    "(x_train, t_train),(x_test,t_test) = load_mnist(flatten=True,normalize=False)\r\n",
    "# pickle功能可以将运行对象保存为文件，下次可以直接使用\r\n",
    "# 函数中使用了pickle功能\r\n",
    "img = x_train[0]\r\n",
    "label = t_train[0]\r\n",
    "print(label)\r\n",
    "print(img.shape)\r\n",
    "img = img.reshape(28,28)\r\n",
    "print(img.shape)\r\n",
    "img_show(img)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 神经网络预测\r\n",
    "# 1*28*28维度的图片对应784个神经元，在255*255*255中选了部分\r\n",
    "# 输入784个神经元，输出0-9的10个神经元\r\n",
    "# 假设隐藏层1有50个神经元，隐藏层2有100个神经元\r\n",
    "\r\n",
    "'''1、获取输入数据\r\n",
    "      2、定义神经网络\r\n",
    "      3、利用神经网络预测 '''\r\n",
    "\r\n",
    "\r\n",
    "def  get_data():\r\n",
    "        (xtrain,t_train),(x_test,t_test) = load_mnist(\r\n",
    "            normalize=True,flatten=True,one_hot_label=False )\r\n",
    "        return x_test,t_test\r\n",
    "    \r\n",
    "def  _network():\r\n",
    "        import pickle\r\n",
    "        with open('sample_weight.pkl', 'rb') as f:\r\n",
    "            network = pickle.load(f)\r\n",
    "        return network\r\n",
    "    \r\n",
    "def predict(network,x):\r\n",
    "        W1,W2,W3  = network['W1'], network['W2'], network['W3']\r\n",
    "        b1,b2,b3 = network['b1'],network['b2'],network['b3']\r\n",
    "        a1 = np.dot(x,W1)+b1\r\n",
    "        z1 = sigmoid(a1)\r\n",
    "        a2 = np.dot(z1,W2) + b2\r\n",
    "        z2 = sigmoid(a2)\r\n",
    "        a3  = np.dot(z2,W3) + b3\r\n",
    "        y =softmax(a3)\r\n",
    "\r\n",
    "        return y\r\n",
    "x,t = get_data()\r\n",
    "network = _network()\r\n",
    "accuracy_cnt = 0\r\n",
    "for i in range(len(x)):\r\n",
    "    y = predict(network,x[i])\r\n",
    "    p = np.argmax(y) \r\n",
    "    if p ==t[i]:\r\n",
    "        accuracy_cnt +=1\r\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt)/len(x)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 所用神经网络的参数特征\r\n",
    "print(x.shape)\r\n",
    "print(x[0].shape)\r\n",
    "print(W1.shape)\r\n",
    "W1,W2,W3  = network['W1'], network['W2'], network['W3']\r\n",
    "print(W1.shape)\r\n",
    "print(W2.shape)\r\n",
    "print(W3.shape) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 因为神经网络只限制列，所以输入100张照片的像素和1张照片的像素无区别\r\n",
    "# 使用batch能够提高计算性能\r\n",
    "x,t = get_data()\r\n",
    "network = _network()\r\n",
    "batch_size = 100\r\n",
    "accuracy_cnt = 0\r\n",
    "\r\n",
    "for i in range(0,len(x),batch_size): # 取0  100 200 读取数据变快\r\n",
    "    x_batch = x[i:i+batch_size]      # 在循环中再取 1 2 101 102  相当于两次循环 \r\n",
    "    y_batch = predict(network,x_batch)\r\n",
    "    p = np.argmax(y_batch,axis=1)\r\n",
    "    accuracy_cnt += np.sum(p==t[i:i+batch_size])\r\n",
    "\r\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt/len(x))))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}